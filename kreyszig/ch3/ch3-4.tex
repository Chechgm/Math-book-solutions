\section{Orthonormal sequences and sets}


\begin{exercise}{1}
Show that an inner product space of finite dimension $n$ has a basis $\set{b_1,\dots, b_n}$ of orthonormal vectors.
(The infinite dimensional case will be considered in section 3.6).
\end{exercise}
\begin{proof}
We know that any vector space of finite dimension has a basis.
We can then apply the Gram-Schmidt procedure, which gives us the desired orthonormal basis.
\end{proof} 

\begin{exercise}{2}
How can we interpret $\sum_{k=0}^n \absoluteValue{\brackets{x, e_k}}^2 \leq \norm{x}^2$ geometrically in $\R^r$, where $r \geq n$?
\end{exercise}
\begin{proof}
Figure 32 and 33 in the book give us a hint of the interpretation of the formula.
The left hand side of the inequality are the projections of $x$ into the subspace spanned by each of the $e_k$, whereas the right hand side is the squared norm of $x$.

Another way I think about the inequality is by thinking of a ``generalisation'' of the Pythagorean Theorem in the plane.
The squared length of the hypotenuse must be equal to the square of its sides.
If we don't consider all of its sides, then the square of the hypotenuse will be larger than the square of some of its sides.
\end{proof} 

\begin{exercise}{3}
Obtain the Schwarz inequality $\absoluteValue{\brackets{x,y}} \leq \norm{x}\norm{y}$ (Section 3.2) from $\sum_{k=0}^n \absoluteValue{\brackets{x, e_k}}^2 \leq \norm{x}^2$.
\end{exercise}
\begin{proof}
Consider the list given by $y/\norm{y}$, since this is a list of length 1, it is orthogonal by definition.
We also have that $\norm{y/\norm{y}} = 1$, so that the list is orthonormal.
Thus
\begin{align*}
    \absoluteValue{\brackets{x, y/\norm{y}}}^2 &\leq \norm{x}^2 &&\iff\\
    \absoluteValue{(1/\norm{y}^2)}\absoluteValue{\brackets{x, y}}^2 &\leq \norm{x}^2 &&\iff\\
    \absoluteValue{\brackets{x, y}}^2 &\leq \norm{x}^2\norm{y}^2,
\end{align*}
as required.
\end{proof} 

\begin{exercise}{4}
Give an example of an $x \in l^2$ such that we have strict inequality in $\sum_{k=0}^\infty \absoluteValue{\brackets{x, e_k}}^2 \leq \norm{x}^2$
\end{exercise}
\begin{proof}
Consider the sequence of orthonormal vectors given by $e_k' = \delta_{j, 2k}$;
that is, the even vectors of the usual $e_k = \delta_{j,k}$.
We have that $(e_n')$ is an orthonormal sequence.
We also have that for any $x \in l^2$, such that $x_{2n} \neq 0$ for any $n$ will give us strict Bessel's inequality.
\end{proof} 

\begin{exercise}{5}
If $(e_k)$ is an orthonormal sequence in an inner product space $X$, and $x \in X$, show that $x-y$ with $y$ given by $y = \sum_{k=1}^n \alpha_k e_k$, with $\alpha_k = \brackets{x, e_k}$ is orthogonal to the subspace $Y_n = \vecspan\set{e_1, \dots, e_n}$.
\end{exercise}
\begin{proof}
First, we have that 
\begin{align*}
    x - y = \sum_{k=n+1}^\infty \alpha_k e_k.
\end{align*}
Now consider $z \in Y_n$, so that $z = \sum_{k=1}^n \beta_k e_k$, with $\beta_k = \brackets{z, e_k}$.
We have that
\begin{align*}
    \brackets{x-y, z}
    = \brackets{\sum_{k=n+1}^\infty \alpha_k e_k, \sum_{j=1}^n \beta_j e_j}
    = \sum_{k=n+1}^\infty \sum_{j=1}^n \alpha_k \beta_j \brackets{e_k,   e_j}
    = 0,
\end{align*}
because $\brackets{e_k, e_j} = 0$ for all $j\neq k$, which is the case here.
Thus $x-y \perp Y_n$, as required.
\end{proof} 

\begin{exercise}{6 (Minimum property of Fourier coefficients)}
Let $\set{e_1, \dots, e_n}$ be an orthonormal set in an inner product space $X$, where $n$ is fixed.
Let $x \in X$ be any fixed element and $y = \beta_1 e_1 + \dots + \beta_n e_n$.
Then $\norm{x-y}$ is minimum if and only if $\beta_j = \brackets{x, e_j}$, where $j=1,\dots, n$.
\end{exercise}
\begin{proof}
($\Rightarrow$)
We have
\begin{align*}
    \norm{x-y} 
    =& \norm{x - \sum_{k=1}^n \brackets{x, e_k}e_k + \sum_{k=1}^n \brackets{x, e_k}e_k - y}\\
    \leq& \norm{x - \sum_{k=1}^n \brackets{x, e_k}e_k} + \norm{\sum_{k=1}^n \brackets{x, e_k}e_k - y}.
\end{align*}
The first term in the right hand side of the last inequality is constant, so we ignore it for minimisation purposes.
Thus, since $\norm{x} \geq 0$ with equality if and only if $x=0$, then $\sum_{k=1}^n \brackets{x, e_k}e_k = y$ so that $\beta_k = \brackets{x, e_k}$.

($\Leftarrow$)
Let $\set{e_{n+1}, e_{n+2},\dots}$ be a sequence of linearly independent vectors in $X$.
Using the Gram-Schmidt procedure, (starting from the given set), construct an orthonormal sequence $(e_k)$ in $X$.
From the previous exercise, we know that if $y = \sum_{k=1}^n \alpha_k e_k$, then $y$ is orthogonal to the complementary space given by $\vecspan(\set{e_{n+1}, e_{n+2},\dots})$.
Thus $\norm{x-y}$ is minimised.
\end{proof} 

\begin{exercise}{7}
Let $(e_k)$ be any orthonormal sequence in an inner product space $X$.
Show that for any $x,y \in X$, $\sum_{k=1}^\infty \absoluteValue{\brackets{x,e_k} \brackets{y, e_k}} \leq \norm{x}\norm{y}$.
\end{exercise}
\begin{proof}
By applying Bessel's inequality to both $x$ and $y$ and multiplying the inequalities together we obtain
\begin{align*}
    \norm{x}^2 \norm{y}^2
    \geq \parens{\sum_k \absoluteValue{\brackets{x, e_k}}^2} \parens{\sum_k \absoluteValue{\brackets{y, e_k}}^2}.
\end{align*}
Taking square root on both sides of the inequality, we have
\begin{align*}
    \norm{x} \norm{y}
    \geq \parens{\sum_k \absoluteValue{\brackets{x, e_k}}^2}^{1/2} \parens{\sum_k \absoluteValue{\brackets{y, e_k}}^2}^{1/2}
    \geq \sum_k \absoluteValue{\brackets{x, e_k} \brackets{y, e_k}},
\end{align*}
where the last inequality follows from the Cauchy-Schwarz inequality (ii in Section 1.2).
\end{proof} 

\begin{exercise}{8}
Show that an element $x$ of an inner product space $X$ cannot have ``too many'' Fourier coefficients $\brackets{x, e_k}$ which are ``big'';
here $(e_k)$ is a given orthonormal sequence;
more precisely, show that the number $n_m$ of $\brackets{x, e_k}$ such that $\absoluteValue{\brackets{x, e_k}} > 1/m$ must satisfy $n_m < m^2\norm{x}^2$.
\end{exercise}
\begin{proof}
Let $M \subseteq \N$ be the set of indices, such that $\absoluteValue{\brackets{x,e_k}} > 1/m$ and by hypothesis the number of elements in $M$ is $n_m$.
From Bessel's inequality, we have
\begin{align*}
    \norm{x}^2
    \geq \sum \absoluteValue{\brackets{x,e_k}}^2
    \geq \sum_{j \in M} \absoluteValue{\brackets{x,e_j}}^2
    > \sum_{j \in M} (1/m)^2
    = n_m/m^2,
\end{align*}
so that $\norm{x}^2 m^2 > n_m$.
\end{proof} 

\begin{exercise}{9}
Orthonomalise the first three terms of the sequence $(x_0, x_1, x_2, \dots)$, where $x_j(t) = t^j$, on the interval $[-1,1]$, where $\brackets{x, y} = \int_{-1}^1 x(t)y(t) dt$.
\end{exercise}
\begin{proof}
We have 
\begin{align*}
    \norm{x_0}^2 = \brackets{x_0,x_0} = \int_{-1}^1 x_0(t)x_0(t) dt = \int_{-1}^1 1 dt = t|_{-1}^1 = 2.
\end{align*}
Thus $e_0 = 1/\sqrt{2}$.

Furthermore, 
\begin{align*}
    \brackets{x_1, e_0} = \int_{-1}^1 t/\sqrt{2} dt = (1/\sqrt{2})\int_{-1}^1 t dt = (1/\sqrt{2}) (t^2/2)|_{-1}^1 = 0.
\end{align*}
Thus $v_1 = t$, and 
\begin{align*}
    \norm{v_1}^2 = \int_{-1}^1 t^2dt = t^3/3|_{-1}^1 = 2/3,
\end{align*}
so that $\norm{v_1} = \sqrt{2/3}$ and $e_1 = \sqrt{2/3}t$.

Finally, 
\begin{align*}
    \brackets{x_2, e_0} 
    = \int_{-1}^1 t^2/\sqrt{2} dt 
    = (1/\sqrt{2})\int_{-1}^1 t^2 dt 
    = (1/\sqrt{2})(2/3),
\end{align*}
and 
\begin{align*}
    \brackets{x_2, e_1} = \int_{-1}^1 \sqrt{2/3}t^3 dt = \sqrt{2/3}\int_{-1}^1 t^3 dt = \sqrt{2/3}[t^4/4]|_{-1}^1 = 0.
\end{align*}
So that $v_2 = x_2 - \brackets{x_2, e_0}e_0 - \brackets{x_2, e_1}e_1 = t^2 - (1/3)$.
Computing the norm of $v_2$ gives us what we need to obtain $e_2$.
\begin{align*}
    \norm{v_2}^2 
    =& \int_{-1}^1 (t^2 - (1/3))^2 dt\\
    =& \int_{-1}^1 t^4 - 2(1/3)t^2 + (1/3))^2 dt\\
    =& \int_{-1}^1 t^4 dt -  \int_{-1}^1 2(1/3)t^2 dt + \int_{-1}^1 (1/3))^2 dt\\
    =& t^5/5|_{-1}^1 - (2/3)(1/3)t^3|_{-1}^1 + (1/3))^2t|_{-1}^1\\
    =& 2/5 - (4/3)(1/3) + 2(1/3))^2\\
    =& 2/5 - 2/9,
\end{align*}
giving us
\begin{align*}
    e_2 
    =& \frac{v_2}{\norm{v_2}}
    = \frac{t^2 - \frac{1}{3}}{\sqrt{\frac{2}{5} - \frac{2}{9}}}
    = \frac{t^2 - \frac{1}{3}}{\sqrt{\frac{8}{45}}}
    = \frac{\frac{3t^2 - 1}{3}}{\frac{\sqrt{\frac{8}{45}}}{1}}
    = \frac{3t^2 - 1}{3\sqrt{\frac{8}{45}}}
    = \frac{3t^2 - 1}{\sqrt{\frac{8}{5}}},
\end{align*}
which is the same as $(5/8)^{1/2}(3t^2 - 1)$.
\end{proof} 
